{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"kernelspec":{"name":"python3","display_name":"Python 3.7.3 64-bit (conda)","metadata":{"interpreter":{"hash":"43bf006da776f8a7689fae5ae832795661e7eb2304ff9fb8aeee6d65c88f33a7"}}},"colab":{"name":"Automatic_Speech_Recognition.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hCcQL1fEjsiX"},"source":["Import libraries"]},{"cell_type":"code","metadata":{"id":"0WSCvLuN9BTN"},"source":["import pandas as pd\n","import numpy as np\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n","import matplotlib.style as ms\n","ms.use('seaborn-muted')\n","%matplotlib nbagg\n","%matplotlib inline\n","import IPython.display as ipd\n","from IPython.display import Audio\n","import os, sys\n","from scipy.io import wavfile\n","from sklearn.preprocessing import LabelEncoder, scale\n","import time\n","import tensorflow as tf\n","tf.config.list_physical_devices('GPU')\n","sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n","from numpy import asarray\n","from numpy import savetxt\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","from tensorflow.keras.utils import to_categorical\n","from keras.utils import np_utils\n","from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, Input, MaxPooling2D, Conv1D, MaxPooling1D, Lambda\n","from tensorflow.keras import losses\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from keras.models import Model, Sequential\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras import backend as K\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PTKkPN69mxn2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"KGZP43yw9BTi"},"source":["path = \"/content/drive/My Drive/Data/Audio Files/\"\n","labels = os.listdir(path)\n","print(labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"v6IRKUaR9BTm"},"source":["ignored_files = {\"_background_noise_\", \"LICENSE\", \"README.md\", \"testing_list.txt\", \"validation_list.txt\", \"Extracted Arrays\",\".ipynb_checkpoints\"}\n","labels = [x for x in os.listdir(path) if x not in ignored_files]\n","print(labels)\n","print(\"Total labels: \", len(labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"kC_C_3af9BTq"},"source":["time_start = time.time()\n","\n","number_of_recordings = []\n","\n","for label in labels:\n","  waves = [f for f in os.listdir(path + '/'+ label) if f.endswith('.wav')]\n","  number_of_recordings.append(len(waves))\n","\n","plt.figure(figsize=(30,5))\n","index = np.arange(len(labels))\n","plt.bar(index, number_of_recordings)\n","plt.xlabel('Commands', fontsize=12)\n","plt.ylabel('No of recordings', fontsize=12)\n","plt.xticks(index, labels, fontsize=15, rotation=60)\n","plt.title('No. of recordings for each command')\n","plt.show()\n","\n","print('Run time: {} mins'.format((time.time()-time_start)/60))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XxaLMfK39BTv"},"source":[""]},{"cell_type":"code","metadata":{"tags":[],"id":"Ngus5MSo9BTw"},"source":["\n","time_start = time.time()\n","\n","duration_of_recordings=[]\n","for label in labels:\n","    print(label)\n","    waves = [f for f in os.listdir(path + '/'+ label) if f.endswith('.wav')]\n","    for wav in waves:\n","        sample_rate, samples = wavfile.read(path + '/' + label + '/' + wav)\n","        duration_of_recordings.append(float(len(samples)/sample_rate))\n","    \n","plt.hist(np.array(duration_of_recordings))\n","\n","print('Run time: {} mins'.format((time.time()-time_start)/60))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dyq2yGmf9BT0"},"source":["savetxt('number_of_recordings.csv', number_of_recordings, delimiter=',')\n","savetxt('duration_of_recordings.csv', duration_of_recordings, delimiter=',')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-eirk9pN9BT4"},"source":["#!cp number_of_recordings.csv \"/content/gdrive/My Drive/Automatic Speech Recognition/Data/Google AI: Speech Commands Dataset/\"\n","#!cp duration_of_recordings.csv \"/content/gdrive/My Drive/Automatic Speech Recognition/Data/Google AI: Speech Commands Dataset/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ms8_BeFd9BT8"},"source":["sum(number_of_recordings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSjtS3gc9BUA"},"source":["path = \"/content/gdrive/My Drive/Automatic Speech Recognition/Data/Google AI: Speech Commands Dataset/Extracted Data/\"\n","path2 = \"/content/gdrive/My Drive/Automatic Speech Recognition/Data/Google AI: Speech Commands Dataset/Extracted Data/Extracted Arrays/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"GM5VvwbR9BUH"},"source":["time_start = time.time()\n","\n","path = \"/content/drive/My Drive/Data/Audio Files/\"\n","path2 = \"/content/drive/My Drive/concatenated_audio/\"\n","\n","for label in labels:\n","\n","    audio_onesec = []\n","    label_list = []\n","    \n","\n","    print(label)\n","    waves = [f for f in os.listdir(path + '/'+ label) if f.endswith('.wav')]\n","\n","    for wav in waves:\n","        samples, sample_rate = librosa.load(path + '/' + label + '/' + wav, sr = 16000)\n","        samples = librosa.resample(samples, 16000, 8000)\n","        if(len(samples) == 8000) : \n","            audio_onesec.append(samples)\n","            label_list.append(label)\n","\n","\n","    np.save(os.path.join(path2, str(label) + '_audio_onesec'), audio_onesec)\n","    np.save(os.path.join(path2, str(label) + '_label_list'), label_list)\n","    print(label, 'is done')\n","\n","print('Run time: {}'.format(time.time()-time_start))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"71wmZb1U9BUc"},"source":["audio_samples1 = np.load(os.path.join(path2, 'bed_audio_onesec.npy'))\n","audio_samples1.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Zfs5rAN9BUf"},"source":["labels1 = np.load(os.path.join(path2, 'bed_label_list.npy'))\n","labels1.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"U8_vXU_e9BUi"},"source":["#concatenat whole datase and labels as well suvh that mapping\n","all_audio = np.empty([0, 8000])\n","\n","for label in labels:\n","  audiosamples = np.load(os.path.join(path2, str(label) + '_audio_onesec.npy'))\n","  #print(audiosamples.shape)\n","  all_audio = np.concatenate((all_audio, audiosamples), axis=0)\n","  print(all_audio.shape)\n","\n","print('done')\n","  \n","np.save(os.path.join(path2, 'all_audio'), all_audio)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"WTci1NLJ9BUq"},"source":["all_labels = np.empty([0,])\n","\n","for label in labels:\n","  current_labels = np.load(os.path.join(path2, str(label) + '_label_list.npy'))\n","  all_labels = np.concatenate((all_labels, current_labels), axis=0)\n","  print(all_labels.shape)\n","\n","print('done')\n","  \n","np.save(os.path.join(path2, 'all_labels'), all_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTBGTsEJ9BUt"},"source":["all_audio = np.load(os.path.join(path2, 'all_audio.npy'))\n","all_labels = np.load(os.path.join(path2, 'all_labels.npy'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"Ov8cNlIP9BUx"},"source":["all_labels = np.reshape(all_labels, (58252, 1))\n","print(all_labels.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"1oomGZYi9BU0"},"source":["all_labels = np.load(os.path.join(path2, 'all_labels.npy'))\n","all_labels_reshaped = np.reshape(all_labels, (58252 , 1))\n","print(all_labels_reshaped.shape)\n","np.save(os.path.join(path2, 'all_labels'), all_labels_reshaped)\n","print('all_labels saved')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"nj94Ir039BU3"},"source":["#one hot encoding\n","all_labels = np.load(os.path.join(path2, 'all_labels.npy'))\n","\n","le = LabelEncoder()\n","y=le.fit_transform(all_labels)\n","classes= list(le.classes_)\n","\n","y = np_utils.to_categorical(y, num_classes=len(labels))\n","\n","print(y.shape)\n","\n","np.save(os.path.join(path2, 'all_labels_encoded'), y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M57Ct0GFCRgy"},"source":["path = \"/content/drive/My Drive/Data/Audio Files/\"\n","path2 = \"/content/drive/My Drive/concatenated_audio/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UxEv4xGP9BU6"},"source":["y = np.load(os.path.join(path2, 'all_labels_encoded.npy'))\n","audio = np.load(os.path.join(path2, 'all_audio.npy'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZ_isPAMJG7P"},"source":["y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lexq637f9BU-"},"source":["audio = np.expand_dims(audio, axis=2)\n","audio.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NDi81rvr9BVC"},"source":["x_train, x_test, Y_train, Y_test = train_test_split(np.array(audio), np.array(y), stratify=y, test_size = 0.3, random_state=32, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9S1DQ2xQY9fD"},"source":["del audio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vdr8ytuu9BVG"},"source":["K.clear_session()\n","\n","num_labels = len(labels)\n","\n","model = Sequential()\n","\n","model.add(Conv1D(filters=512, kernel_size=3, padding='valid', activation='relu', input_shape=(8000, 1)))\n","model.add(MaxPooling1D(pool_size=3))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n","model.add(MaxPooling1D(pool_size=3))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n","model.add(MaxPooling1D(pool_size=3))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))                                \n","model.add(MaxPooling1D(pool_size=3))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n","model.add(MaxPooling1D(pool_size=3))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n","model.add(MaxPooling1D(pool_size=3))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n","model.add(MaxPooling1D(pool_size=3))\n","model.add(Dropout(0.2))\n","\n","\n","model.add(Flatten())\n","\n","model.add(Dense(256, activation='relu'))\n","\n","model.add(Dense(num_labels, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"FgCXg4nx9BVK"},"source":["model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"IB8jY9Rl9BVO"},"source":["batch_size = 128\n","num_epochs = 240\n","\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from keras.callbacks import TensorBoard, CSVLogger\n","from keras.models import load_model\n","from numpy.testing import assert_allclose\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)  \n","path_checkpoint = \"H:\\whizkey\\FINAL PAPER/weights_of_audio.best.hdf5\"\n","mc = ModelCheckpoint(path_checkpoint, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n","\n","tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0,\n","                          write_graph=True, write_images=True)\n","\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, \n","                              mode='auto',min_delta=0.0001, min_lr=0.0000001)\n","\n","csv_logger = CSVLogger('H:\\whizkey\\FINAL PAPER/csv_logger_audio.csv', append=True, separator=',')\n","\n","cbks = [reduce_lr, tensorboard, es, csv_logger, mc]\n","\n","history = model.fit(x_train, Y_train, batch_size=batch_size, epochs=num_epochs,\n","                    callbacks = cbks, verbose=1, validation_data=(x_test, Y_test))\n","\n","print('Run time: {} mins'.format((time.time()-time_start)/60))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m_hzz1nH9BVR"},"source":["score = model.evaluate(x_train, Y_train, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = model.evaluate(x_test, Y_test, verbose=0)\n","print(\"Testing Accuracy: \", score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q7IB5nylgYRw"},"source":["Below code is for Speech Recgonition: To be used by developer only (Windows OS), for end-user \"Automatic_Speech_Recognition_End_User.ipynb\""]},{"cell_type":"markdown","metadata":{"id":"jlFXsmv7Ax5p"},"source":["Load Weights"]},{"cell_type":"code","metadata":{"id":"6LH2TVHvA3dR"},"source":["from keras.models import load_model\n","model = load_model('H:\\whizkey\\FINAL PAPER\\weights_of_audio.best.hdf5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gknMeOz4A1Ee"},"source":["score = model.evaluate(x_train, Y_train, verbose=0)\n","print(\"Training Accuracy: \", score[1])\n","\n","score = model.evaluate(x_test, Y_test, verbose=0)\n","print(\"Testing Accuracy: \", score[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OvXMujPwtH2T"},"source":["print(x_test.shape)\n","print(x_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"77vrRPcXgYRx"},"source":["labels = ['eight', 'sheila', 'nine', 'yes', 'one', 'no', 'left', 'tree', 'bed', 'bird', 'go', 'wow', 'seven', 'marvin', 'dog', 'three', 'two', 'house', 'down', 'six', 'five', 'off', 'right', 'cat', 'zero', 'four', 'stop', 'up', 'on', 'happy']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNOx3kBEC00E"},"source":["all_labels = np.load(os.path.join(\"all_labels.npy\"))\n","\n","le = LabelEncoder()\n","y=le.fit_transform(all_labels)\n","classes= list(le.classes_)\n","\n","y = np_utils.to_categorical(y, num_classes=len(labels))\n","\n","print(y.shape)\n","\n","#np.save(os.path.join(path2, 'all_labels_encoded'), y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mPgds_cuQSF_"},"source":["Predict probabilities for all speeches.\n","Maximum probabilty with a threshold of 0.5 is chosen as predicted speech."]},{"cell_type":"code","metadata":{"id":"iNgxMI_WCNnq"},"source":["from datetime import datetime \n","import pytz \n","import csv\n","import random\n","  \n","# get the standard UTC time  \n","UTC = pytz.utc \n","IST = pytz.timezone('Asia/Kolkata') \n","\n","def predict(audio,n):\n","    prob=model.predict_proba(audio.reshape(1,8000,1))\n","    index=np.argmax(prob[0])\n","    #print(prob[0])  #Print all probabilities\n","    #print(max(prob[0]))     #Print max probability\n","    if max(prob[0]) > 0.5:\n","        #print(index)\n","        f=open(r'H:\\\\whizkey\\\\FINAL PAPER\\\\Excel files\\\\Texter' + '{0}'.format(n)+'.txt', 'a')\n","        f.write(str(datetime.now(IST)) + \",\" + classes[index] + \"\\n\")\n","        f.close()\n","        return classes[index]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SaaSeP5HQhri"},"source":["Predictions on random speech commands from test set."]},{"cell_type":"code","metadata":{"id":"5Td4n0HXCcWZ"},"source":["# Dont run\n","import random\n","index=random.randint(0,len(x_test)-1)\n","print(index)\n","samples=x_test[index].ravel()\n","print(\"Audio:\",classes[np.argmax(Y_test[index])])\n","ipd.Audio(samples, rate=8000)\n","print(\"Text:\",predict(samples))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"85E8Ec23QobK"},"source":["Dependencies for Silence Detection"]},{"cell_type":"code","metadata":{"id":"KA92SIUwDLLs"},"source":["!pip install pydub &> /dev/null\n","!pip install sounddevice &> /dev/null\n","!pip install soundfile &> /dev/null\n","!sudo apt-get install libportaudio2 &> /dev/null\n","!pip install pysndfx &> /dev/null\n","!pip install python_speech_features &> /dev/null\n","#!sudo apt-get install sox &> /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xqpnVdeDL2T"},"source":["# Import the AudioSegment class for processing audio and the \n","# split_on_silence function for separating out silent chunks.\n","from pydub import AudioSegment\n","from pydub.silence import split_on_silence\n","\n","# Define a function to normalize a chunk to a target amplitude.\n","def match_target_amplitude(aChunk, target_dBFS):\n","    ''' Normalize given audio chunk '''\n","    change_in_dBFS = target_dBFS - aChunk.dBFS\n","    return aChunk.apply_gain(change_in_dBFS)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJkVvV7SgYRy"},"source":["from pathlib import Path\n","AudioSegment.converter = r\"H:\\\\whizkey\\\\FINAL PAPER\\\\ffmpeg\\\\ffmpeg.exe\"\n","AudioSegment.ffprobe   = r\"H:\\\\whizkey\\\\FINAL PAPER\\\\ffmpeg\\\\ffprobe.exe\"\n"," \n","print (AudioSegment.converter)\n","print (AudioSegment.ffprobe)\n"," \n","my_file = Path(r\"H:\\\\whizkey\\\\FINAL PAPER\\\\cont speech\\\\newer10.mp3\")\n","print ('ID1 : %s' % my_file) \n","audio = AudioSegment.from_file_using_temporary_files(my_file)    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5f0Gn0sYDcFT"},"source":["from os import path\n","from pydub import AudioSegment\n","\n","# files                                                                         \n","src = r\"H:\\\\whizkey\\\\FINAL PAPER\\\\cont speech\\\\newer10.mp3\"\n","dst = r\"H:\\\\whizkey\\\\FINAL PAPER\\\\cont speech\\\\newer10.wav\"\n","\n","# convert wav to mp3                                                            \n","song1 = AudioSegment.from_file_using_temporary_files(src)\n","song1.export(dst, format=\"wav\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S78iBAPxQ2Bf"},"source":["Calculate dBFS (Decibels relative to full scale)"]},{"cell_type":"code","metadata":{"id":"wHQ4YfudDjAj"},"source":["dBFS = song1.dBFS"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGGZT1AXDj1K"},"source":["chunks = split_on_silence(\n","    song1,\n","    min_silence_len = 200,          #Set minimum silence length threshold to 100 ms\n","    silence_thresh = dBFS-16\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-UVEqCBiRHkI"},"source":["Split the continuous audio into individual speech commands."]},{"cell_type":"code","metadata":{"id":"cWyYOKI9DlxQ"},"source":["# Process each chunk with your parameters\n","n = random. randint(0,9999999999999999999999) \n","\n","for i, chunk in enumerate(chunks):\n","    # Create a silence chunk that's 0.5 seconds (or 500 ms) long for padding.\n","    silence_chunk = AudioSegment.silent(duration=200)\n","\n","    # Add the padding chunk to beginning and end of the entire chunk.\n","    audio_chunk = silence_chunk + chunk + silence_chunk\n","\n","    # Normalize the entire chunk.\n","    normalized_chunk = match_target_amplitude(audio_chunk, -20.0)\n","\n","    # Export the audio chunk with new bitrate.\n","    print(\"Exporting chunker{0}.wav.\".format(i))\n","    k = normalized_chunk.export(\n","        r'H:\\\\whizkey\\\\FINAL PAPER\\\\Chunkers\\\\chunker{0}.wav' .format(i),\n","        bitrate = \"192k\",\n","        format = \"wav\"\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sZtq-W5JRSrl"},"source":["Resample each speech command to 8000 Hz"]},{"cell_type":"code","metadata":{"id":"9JqUzVFcDn3I"},"source":["time_start = time.time()\n","ctr = 0\n","for i, chunk in enumerate(chunks):\n","    ctr += 1\n","    melspec1 = []\n","    filepath=r'H:\\\\whizkey\\\\FINAL PAPER\\\\Chunkers\\\\chunker{0}.wav'.format(i)\n","    print(filepath)\n","    \n","    samples, sample_rate = librosa.load(filepath, sr = 16000)\n","    samples = librosa.resample(samples, 16000, 8000)\n","    samples.resize(8000)\n","\n","    pred = predict(samples,n)         #Prediction after resampling to 8000Hz    \n","    print(\"Text:\",pred)\n","\n","print('Run time: {}'.format(time.time()-time_start))\n","print('Total speech commands: ',ctr)\n","dataframe1 = pd.read_csv(r\"H:\\\\whizkey\\\\FINAL PAPER\\\\Excel files\\\\texter{0}.txt\".format(n),header=None) \n","dataframe1.to_csv(r\"H:\\\\whizkey\\\\FINAL PAPER\\\\Excel files\\\\texter{0}.csv\".format(n),index = None)\n","print(\"H:\\\\whizkey\\\\FINAL PAPER\\\\Excel files\\\\texter{0}.csv\".format(n))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"infvFI3CRiI_"},"source":["Play individual speech command"]},{"cell_type":"code","metadata":{"id":"fZA-_AP7EV8x"},"source":["ipd.Audio('/content/chunker38.wav', rate=8000)"],"execution_count":null,"outputs":[]}]}